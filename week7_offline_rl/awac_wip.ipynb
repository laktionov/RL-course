{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slz3MlAcDi7M"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !pip -q install gymnasium[mujoco]\n",
        "    !pip -q install minari\n",
        "    !pip -q install gymnasium-robotics\n",
        "    !pip -q install torchrl\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dYyoG02v5CIb"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_SiZi64EbAs",
        "outputId": "66a168a3-9b8a-4808-8324-f634fe00801d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e649534d590>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gymnasium import spaces\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import minari\n",
        "from minari import DataCollector\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nRDHdE-hUS1"
      },
      "source": [
        "[Minari](https://minari.farama.org/main/) is a Python API that hosts a collection of popular Offline Reinforcement Learning datasets. The environments from which these datasets are generated follow the [Gymnasium API](https://gymnasium.farama.org/). The datasets are publicly available in a [Farama GCP bucket](https://console.cloud.google.com/storage/browser/minari-datasets;tab=objects?forceOnBucketsSortingFiltering=false&amp;project=mcmes-345620&amp;prefix=&amp;forceOnObjectsSortingFiltering=false) and can be downloaded through the Minari CLI. Minari also provides dataset handling features such as episode sampling, filtering trajectories, as well as dataset generation utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q2GL3mw1RWj"
      },
      "outputs": [],
      "source": [
        "dataset = minari.load_dataset(\"door-expert-v1\", download=True)\n",
        "env = dataset.recover_environment(render_mode=\"rgb_array\")\n",
        "env.reset()\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VPmBwVMrqmX",
        "outputId": "0aae8dca-558c-4890-a86d-460913f31ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "observation_space: Box(-inf, inf, (39,), float64)\n",
            "action_space: Box(-1.0, 1.0, (28,), float32)\n"
          ]
        }
      ],
      "source": [
        "observation_space = env.observation_space\n",
        "action_space = env.action_space\n",
        "print(f\"observation_space: {observation_space}\")\n",
        "print(f\"action_space: {action_space}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AoVOjweyM0vX"
      },
      "outputs": [],
      "source": [
        "state_dim = observation_space.shape[0]\n",
        "action_dim = action_space.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ywW1hfAh4Fu"
      },
      "source": [
        "[TorchRL](https://pytorch.org/rl/) is an open-source Reinforcement Learning library for PyTorch. TorchRL provides pytorch and python-first, low and high level abstractions for RL that are intended to be efficient, modular, documented and properly tested. The code is aimed at supporting research in RL. Most of it is written in python in a highly modular way, such that researchers can easily swap components, transform them or write new ones with little effort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lJ0T_OEAHiL"
      },
      "outputs": [],
      "source": [
        "from torchrl.data.datasets.minari_data import MinariExperienceReplay\n",
        "from torchrl.data.replay_buffers import SamplerWithoutReplacement\n",
        "from torchrl.envs import DoubleToFloat\n",
        "\n",
        "dataset_id = \"door-expert-v1\"\n",
        "batch_size = 256\n",
        "\n",
        "replay_buffer = MinariExperienceReplay(\n",
        "    dataset_id,\n",
        "    split_trajs=False,\n",
        "    batch_size=batch_size,\n",
        "    sampler=SamplerWithoutReplacement(),\n",
        "    transform=DoubleToFloat(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGa5mn1Gg00E"
      },
      "source": [
        "## Introduction\n",
        "**Advantage Weighted Actor-Critic (AWAC)** is a novel approach in the domain of offline reinforcement learning that aims to efficiently utilize a fixed dataset of experiences to learn optimal policies. The core idea behind AWAC is to leverage the advantage function to guide the policy improvement step, ensuring that the updated policy favors actions that lead to higher-than-average returns.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "### Advantage Function\n",
        "The advantage function, $A(s, a)$, measures the relative benefit of taking an action $a$ in state $s$ over the average action at that state. It is defined as the difference between the Q-value of the action and the value function of the state:\n",
        "\n",
        "$$\n",
        "A(s, a) = Q(s, a) - V(s)\n",
        "$$\n",
        "\n",
        "### Actor-Critic Architecture\n",
        "AWAC utilizes the actor-critic architecture, where the \"actor\" refers to the policy $\\pi(a|s)$ that selects actions based on the current state, and the \"critic\" estimates the value function $V(s)$ or the action-value function $Q(s, a)$. This architecture enables efficient policy improvement and value estimation.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mHIB77FX2sq4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions.transforms import TanhTransform\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.distributions.transformed_distribution import TransformedDistribution\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.backbone = nn.Sequential(\n",
        "            #YOUR_CODE\n",
        "        )\n",
        "\n",
        "        self.mu_head = nn.Linear(128, action_dim)\n",
        "        self.log_sigma = nn.Parameter(torch.zeros(action_dim, dtype=torch.float32))\n",
        "\n",
        "    def forward(self, state):\n",
        "        features = self.backbone(state)\n",
        "        mu = #YOUR_CODE\n",
        "        sigma = #YOUR_CODE\n",
        "        return mu, sigma\n",
        "\n",
        "    def get_distribution(self, state):\n",
        "        mu, sigma = self.forward(state)\n",
        "        return #YOUR_CODE\n",
        "\n",
        "    def rsample(self, state):\n",
        "        return #YOUR_CODE\n",
        "    @torch.no_grad()\n",
        "    def sample(self, state):\n",
        "        return #YOUR_CODE\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_action(self, state):\n",
        "        return self.forward(state)[0]\n",
        "\n",
        "\n",
        "class QCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            #YOUR_CODE\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        return #YOUR_CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7VYqBcsrQxOr"
      },
      "outputs": [],
      "source": [
        "def evaluate(env, actor, n_games=10, t_max=10000):\n",
        "    rewards = []\n",
        "    for _ in range(n_games):\n",
        "        s, _ = env.reset()\n",
        "        reward = 0\n",
        "        for _ in range(t_max):\n",
        "            action = actor.get_action(torch.tensor([s], dtype=torch.float32)).numpy()[0]\n",
        "            s, r, terminated, truncated, _ = env.step(action)\n",
        "            reward += r\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        rewards.append(reward)\n",
        "    return np.mean(rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "L8NpV4a2RF5h"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "from IPython.display import clear_output\n",
        "\n",
        "plt.rcParams[\"axes.grid\"] = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2p1dPp5gUQr"
      },
      "source": [
        "## AWAC Algorithm\n",
        "\n",
        "1. **Initialization**: Start with an initial policy $\\pi_0$ and a dataset of experiences $D$ collected under some behavior policy.\n",
        "\n",
        "2. **Critic Update**: Use the dataset $D$ to update the critic by minimizing the difference between the predicted Q-values and the observed rewards plus the discounted value of the next state. This step can be formulated as minimizing the loss:\n",
        "\n",
        "$$\n",
        "L_{critic} = \\mathbb{E}_{(s, a, r, s') \\sim D}[(Q(s, a) - (r + \\gamma V(s')))^2]\n",
        "$$\n",
        "\n",
        "3. **Advantage Calculation**: Calculate the advantage $A(s, a) = Q(s, a) - Q(s, a_{new}), a_{new} \\sim \\pi_\\theta(.|s)$ using the updated critic for actions in the dataset.\n",
        "\n",
        "4. **Actor Update**: Improve the policy by re-weighting the likelihood of actions in proportion to their advantage. This step encourages the selection of actions with higher-than-average returns. The policy is updated by optimizing:\n",
        "\n",
        "$$\n",
        "L_{actor} = -\\mathbb{E}_{(s, a) \\sim D}[\\log \\pi(a|s) \\cdot \\exp(\\dfrac{A(s, a)}{\\lambda})]\n",
        "$$\n",
        "\n",
        "where $\\lambda$ is a temperature parameter that controls the importance of the advantage in the policy update.\n",
        "\n",
        "5. **Iterate**: Repeat the critic update and actor update steps until the policy converges or a predetermined number of iterations is reached.\n",
        "\n",
        "## Incorporating the Twin Trick in AWAC\n",
        "\n",
        "### Overestimation Bias in Q-Learning\n",
        "A common challenge in Q-learning and its variants, including those used in actor-critic architectures, is the overestimation of Q-values. This bias occurs because the max operator in the Q-learning update tends to select overoptimistic value estimates, leading to unstable training and suboptimal policies.\n",
        "\n",
        "### The Twin Trick Solution\n",
        "To mitigate the overestimation bias, AWAC employs a technique known as the \"twin trick.\" This involves maintaining two separate critic networks, $Q_{\\phi_1}(s, a)$ and $Q_{\\phi_2}(s, a)$, with their own sets of parameters $\\theta_1$ and $\\theta_2$. These networks are trained on the same experiences but initialized differently to provide independent estimates of the action-value function.\n",
        "\n",
        "### Critic Update with the Twin Trick\n",
        "During the critic update step, both critic networks are updated using the observed transitions. However, when calculating the target value for updating each critic, AWAC uses the minimum of the two Q-value estimates from the twin critics for the next state-action pair:\n",
        "\n",
        "$$\n",
        "y = r + \\gamma \\min_{i=1,2} Q_{\\phi^-_i}(s', \\pi(s'))\n",
        "$$\n",
        "\n",
        "This target $y$ is then used to update both critic networks. By taking the minimum of the two estimates, the twin trick effectively reduces the likelihood of overestimation bias because it is less probable for both networks to overestimate the Q-value of the same action simultaneously.\n",
        "\n",
        "### Advantages of the Twin Trick in AWAC\n",
        "\n",
        "- **Reduced Overestimation Bias**: By using the minimum of the Q-value estimates from two critics, AWAC limits the impact of overestimation, leading to more accurate value estimates and more stable learning.\n",
        "- **Improved Policy Performance**: With more reliable critic estimates, the policy update step can make better-informed adjustments, potentially leading to higher-quality policies.\n",
        "- **Enhanced Stability**: The twin trick contributes to the overall stability of the AWAC algorithm, making it more robust against the fluctuations that can occur in offline RL settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "jB_jrL72IK3u",
        "outputId": "cdef1552-071d-4bf7-cdc1-3a903167d825"
      },
      "outputs": [],
      "source": [
        "actor_losses = []\n",
        "critic_losses = []\n",
        "episode_rewards = []\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "hidden_dim = 128\n",
        "actor = Actor(state_dim, action_dim, hidden_dim).to(device)\n",
        "\n",
        "critic1 = QCritic(state_dim, action_dim, hidden_dim).to(device)\n",
        "target_critic1 = QCritic(state_dim, action_dim, hidden_dim).to(device)\n",
        "target_critic1.load_state_dict(critic1.state_dict())\n",
        "\n",
        "critic2 = QCritic(state_dim, action_dim, hidden_dim).to(device)\n",
        "target_critic2 = QCritic(state_dim, action_dim, hidden_dim).to(device)\n",
        "target_critic2.load_state_dict(critic2.state_dict())\n",
        "\n",
        "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
        "critic1_optimizer = torch.optim.Adam(critic1.parameters(), lr=3e-4)\n",
        "critic2_optimizer = torch.optim.Adam(critic2.parameters(), lr=3e-4)\n",
        "\n",
        "\n",
        "iterations = 100000\n",
        "gamma = 0.99\n",
        "lagrange_multiplier = 1.0\n",
        "eval_freq = 500\n",
        "tau = 0.005\n",
        "eps = 1e-6\n",
        "\n",
        "low, high = torch.tensor(env.action_space.low, dtype=torch.float32), torch.tensor(\n",
        "    env.action_space.high, dtype=torch.float32\n",
        ")\n",
        "\n",
        "for step in trange(iterations):\n",
        "    # Sample a batch from the dataset\n",
        "    batch = replay_buffer.sample()\n",
        "    state = batch[\"observation\"].to(device)\n",
        "    action = torch.clamp(batch[\"action\"], low + eps, high - eps).to(device)\n",
        "    reward = batch[\"next\"][\"reward\"].to(device)\n",
        "    next_state = batch[\"next\"][\"observation\"].to(device)\n",
        "    not_done = ~batch[\"next\"][\"done\"].to(device)\n",
        "\n",
        "    actor.to(device)\n",
        "    # Critic update\n",
        "    with torch.no_grad():\n",
        "        next_action = #YOUR_CODE\n",
        "        next_q_value = #YOUR_CODE\n",
        "        target_q_value = #YOUR_CODE\n",
        "    current_q_value1 = critic1(state, action)\n",
        "    current_q_value2 = critic2(state, action)\n",
        "    critic_loss = #YOUR_CODE\n",
        "\n",
        "    critic1_optimizer.zero_grad()\n",
        "    critic2_optimizer.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic1_optimizer.step()\n",
        "    critic2_optimizer.step()\n",
        "\n",
        "    # Actor update\n",
        "    with torch.no_grad():\n",
        "        sampled_action = #YOUR_CODE\n",
        "        q = #YOUR_CODE\n",
        "        v = #YOUR_CODE\n",
        "        advantage = #YOUR_CODE\n",
        "        weights = torch.clamp(#YOUR_CODE, -100, 100)\n",
        "\n",
        "    distribution = actor.get_distribution(state)\n",
        "    log_prob = #YOUR_CODE\n",
        "    actor_loss = #YOUR_CODE\n",
        "\n",
        "    actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optimizer.step()\n",
        "\n",
        "    actor_losses.append(actor_loss.cpu().item())\n",
        "    critic_losses.append(critic_loss.cpu().item())\n",
        "\n",
        "    for param, target_param in zip(critic1.parameters(), target_critic1.parameters()):\n",
        "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "    for param, target_param in zip(critic2.parameters(), target_critic2.parameters()):\n",
        "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "    if step % eval_freq == 0:\n",
        "        episode_rewards.append(evaluate(env, actor.cpu(), n_games=5))\n",
        "\n",
        "        # Plotting the results\n",
        "        clear_output(wait=True)\n",
        "        plt.figure(figsize=(20, 5))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(episode_rewards)\n",
        "        plt.title(\"Episode Reward\")\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Reward\")\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(actor_losses)\n",
        "        plt.title(\"Actor Loss\")\n",
        "        plt.xlabel(\"Step\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(critic_losses)\n",
        "        plt.title(\"Critic Loss\")\n",
        "        plt.xlabel(\"Step\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5G_FKabEXgn",
        "outputId": "4009e655-a55e-491c-8c2f-11f543c060ff"
      },
      "outputs": [],
      "source": [
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "# let's hope this will work\n",
        "# don't forget to pray\n",
        "with RecordVideo(env=env, video_folder=\"./videos\") as env_monitor:\n",
        "    evaluate(env_monitor, actor.cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud8dli-ozvMS"
      },
      "outputs": [],
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from base64 import b64encode\n",
        "from IPython.display import HTML\n",
        "import sys\n",
        "\n",
        "video_paths = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "video_path = video_paths[0]  # You can also try other indices\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    # https://stackoverflow.com/a/57378660/1214547\n",
        "    with video_path.open('rb') as fp:\n",
        "        mp4 = fp.read()\n",
        "    data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "else:\n",
        "    data_url = str(video_path)\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(data_url))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XoWgw9i7Wdz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
