{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQqLJOESzeit"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !pip -q install gymnasium\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISsSlsh8zhiU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "hH5JznwNccCk",
        "outputId": "049ce5f8-48b1-4721-a762-878ad711e738"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
        "env.reset()\n",
        "plt.imshow(env.render())\n",
        "\n",
        "action_dim = env.action_space.shape[0]\n",
        "state_dim = env.observation_space.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6blsEEcVJ6he"
      },
      "source": [
        "The **Deep Deterministic Policy Gradient (DDPG)** algorithm is a model-free, off-policy actor-critic algorithm designed for environments with continuous action spaces. It integrates the concepts of Q-learning and policy improvement to provide a framework for learning optimal policies.\n",
        "\n",
        "For a policy $\\mu_{\\theta}(s)$ parameterized by $\\theta$, the gradient of the expected return $J$ with respect to the actor parameters $\\theta$ is given by:\n",
        "\\begin{equation}\n",
        "\\nabla_{\\theta} J = \\mathbb{E}_{s \\sim d^\\mu} \\left[\\nabla_{\\theta} \\mu_{\\theta}(s) \\nabla_{a} Q^{\\mu}(s, a) \\big|_{a=\\mu_{\\theta}(s)}\\right]\n",
        "\\end{equation}\n",
        "where $Q^{\\mu}(s, a)$ is the action-value function under policy $\\mu$, and $d^\\mu$ is the state distribution under policy $\\mu$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm3-5ndZKIAW"
      },
      "source": [
        "**Actor-Critic Architecture:**\n",
        "\n",
        "DDPG uses an actor-critic architecture, where the actor network approximates the optimal policy deterministically, outputting the best believed action for any given state. The critic evaluates the expected return of the state-action pair, providing a gradient for updating the actor's policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YCi24uLd8Jw"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super().__init__()\n",
        "        self.model = #YOUR_CODE\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, state):\n",
        "        return #YOUR_CODE\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.model = #YOUR_CODE\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        return #YOUR_CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T065nsxCKRsR"
      },
      "source": [
        "**Experience Replay:**\n",
        "\n",
        "To break the correlation between consecutive samples and to utilize the learning data more efficiently, DDPG implements experience replay. This technique stores a buffer of previous state-action-reward-next state tuples, sampling from this buffer randomly to train the networks. This process not only stabilizes training but also allows for the reuse of past experiences, improving sample efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekVSiSWweGbn"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=int(1e6)):\n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.storage)\n",
        "\n",
        "    def add(self, transition):\n",
        "        if len(self.storage) == self.max_size:\n",
        "            #YOUR_CODE\n",
        "        else:\n",
        "            self.storage.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "        batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = (\n",
        "            [],\n",
        "            [],\n",
        "            [],\n",
        "            [],\n",
        "            [],\n",
        "        )\n",
        "        for i in indices:\n",
        "            state, action, reward, next_state, done = self.storage[i]\n",
        "            batch_states.append(np.array(state, copy=False))\n",
        "            batch_actions.append(np.array(action, copy=False))\n",
        "            batch_rewards.append(np.array(reward, copy=False))\n",
        "            batch_next_states.append(np.array(next_state, copy=False))\n",
        "            batch_dones.append(np.array(done, copy=False))\n",
        "        return (\n",
        "            np.array(batch_states),\n",
        "            np.array(batch_actions),\n",
        "            np.array(batch_rewards).reshape(-1, 1),\n",
        "            np.array(batch_next_states),\n",
        "            np.array(batch_dones).reshape(-1, 1),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFtp5D4nKYWk"
      },
      "source": [
        "**Target Networks:**\n",
        "\n",
        "DDPG employs target networks for both the actor and the critic to stabilize training. These target networks are copies of the actor and critic networks that are slowly updated towards the learned networks. By using target networks, DDPG mitigates the risk of the moving targets problem, where updates are based on moving estimates, thereby enhancing learning stability.\n",
        "\n",
        "The target networks are updated using a soft update strategy, which slowly tracks the learned networks. The update rule for the target networks is given by:\n",
        "\\begin{equation}\n",
        "\\theta^- = \\tau \\theta + (1 - \\tau) \\theta^-\n",
        "\\end{equation}\n",
        "where $\\theta^-$ are the parameters of the target network, $\\theta$ are the parameters of the corresponding learned network, and $\\tau \\ll 1$ controls the rate of update."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB_rsQOOK2LC"
      },
      "source": [
        "**Exploration:**\n",
        "\n",
        "For exploration, noise is added to the actor's output:\n",
        "\\begin{equation}\n",
        "a = \\mu_{\\theta}(s) + \\mathcal{N}\n",
        "\\end{equation}\n",
        "where $\\mathcal{N}$ is a noise process, such as the Gaussian or Ornstein-Uhlenbeck process, designed to provide temporal correlation between successive actions, facilitating efficient exploration in continuous action spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Js7p5AdgKUn"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "max_action = float(env.action_space.high[0])\n",
        "actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "target_actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "target_actor.load_state_dict(actor.state_dict())\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "\n",
        "critic = Critic(state_dim, action_dim).to(device)\n",
        "target_critic = Critic(state_dim, action_dim).to(device)\n",
        "target_critic.load_state_dict(critic.state_dict())\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "\n",
        "def get_action(actor, state, low, high, std_noise=0):\n",
        "    state = torch.tensor(state.reshape(1, -1), dtype=torch.float32, device=device)\n",
        "    action = actor(state).cpu().detach().numpy().flatten()\n",
        "    return #YOUR_CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR_u-B_zLs_v"
      },
      "source": [
        "**Critic update:**\n",
        "\n",
        "The critic is updated by minimizing the MSE between the predicted Q-values and the target Q-values. The target Q-value for a given state-action pair $(s, a)$ is computed as:\n",
        "\n",
        "$y = r + \\gamma Q_{\\phi^-}(s', \\mu_{\\theta^-}(s'))$\n",
        "\n",
        "where $r$ is the reward received after executing action $a$ in state $s$, $\\gamma$ is the discount factor, $s'$ is the next state, $\\mu_{\\theta^-}$ is the target policy, and $Q_{\\phi^-}$ is the target critic network. The loss for the critic is then:\n",
        "\n",
        "$L(\\phi) = (y - Q_\\phi(s, a))^2 \\to \\min_\\phi$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBiiNZ0uNGHC"
      },
      "source": [
        "**Actor update:**\n",
        "\n",
        "The actor network is updated by maximising the critic's estimation:\n",
        "\n",
        "$J(\\theta) = Q_\\phi(s, \\mu_\\theta(s)) \\to \\max_\\theta$\n",
        "\n",
        "This equation uses the chain rule to compute the gradient of the policy's performance with respect to the actor's parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8jWTxEbeIsM"
      },
      "outputs": [],
      "source": [
        "def train_step(batch_size=100, gamma=0.99, tau=0.005):\n",
        "    # Sample replay buffer\n",
        "    state, action, reward, next_state, is_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
        "    action = torch.tensor(action, dtype=torch.float32, device=device)\n",
        "    next_state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
        "    is_not_done = torch.tensor(1 - is_done, dtype=torch.float32, device=device)\n",
        "    reward = torch.tensor(reward, dtype=torch.float32, device=device)\n",
        "\n",
        "    # Compute the target Q value\n",
        "    with torch.no_grad():\n",
        "        target_Q = #YOUR_CODE\n",
        "\n",
        "    # Get current Q estimate\n",
        "    pred_Q = #YOUR_CODE\n",
        "\n",
        "    # Compute critic loss\n",
        "    assert target_Q.shape == pred_Q.shape\n",
        "    critic_loss = #YOUR_CODE\n",
        "\n",
        "    # Optimize the critic\n",
        "    critic_optimizer.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic_optimizer.step()\n",
        "\n",
        "    # Compute actor loss\n",
        "    actor_loss = #YOUR_CODE\n",
        "\n",
        "    # Optimize the actor\n",
        "    actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    actor_optimizer.step()\n",
        "\n",
        "    # Update the frozen target models\n",
        "    for param, target_param in zip(critic.parameters(), target_critic.parameters()):\n",
        "        #YOUR_CODE\n",
        "\n",
        "    for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
        "        #YOUR_CODE\n",
        "\n",
        "    return actor_loss.item(), critic_loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTU6Xm7lmvYH"
      },
      "outputs": [],
      "source": [
        "def evaluate(env, actor, low, high, n_games=1, t_max=10000):\n",
        "    rewards = []\n",
        "    for _ in range(n_games):\n",
        "        s, _ = env.reset()\n",
        "        reward = 0\n",
        "        for _ in range(t_max):\n",
        "            action = get_action(actor, s, low, high)\n",
        "            s, r, terminated, truncated, _ = env.step(action)\n",
        "            reward += r\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        rewards.append(reward)\n",
        "    return np.mean(rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmzF5OkXkdE8"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "from IPython.display import clear_output\n",
        "\n",
        "plt.rcParams[\"axes.grid\"] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "DMdBKG3ZeKAX",
        "outputId": "c94aef59-4705-463d-82fc-4ef2edcc6164"
      },
      "outputs": [],
      "source": [
        "actor_losses = []\n",
        "critic_losses = []\n",
        "episode_rewards = []\n",
        "\n",
        "replay_buffer = ReplayBuffer(int(1e6))\n",
        "\n",
        "total_steps = 100000\n",
        "batch_size = 100\n",
        "eval_freq = 500\n",
        "tau = 0.005\n",
        "std_noise = 0.1\n",
        "gamma = 0.99\n",
        "\n",
        "low, high = env.action_space.low[0], env.action_space.high[0]\n",
        "\n",
        "max_action = env.action_space.high\n",
        "\n",
        "state, _ = env.reset()\n",
        "for step in trange(1, total_steps + 1):\n",
        "\n",
        "    action = get_action(actor, state, low, high, std_noise=std_noise)\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    replay_buffer.add((state, action, reward, next_state, done))\n",
        "    state = next_state\n",
        "\n",
        "    if len(replay_buffer) >= batch_size:\n",
        "        actor_loss, critic_loss = train_step(batch_size, gamma, tau)\n",
        "        actor_losses.append(actor_loss)\n",
        "        critic_losses.append(critic_loss)\n",
        "\n",
        "    if done:\n",
        "        state, _ = env.reset()\n",
        "\n",
        "    if step % eval_freq == 0:\n",
        "        episode_rewards.append(\n",
        "            evaluate(gym.make(\"Pendulum-v1\"), actor, low, high, n_games=10)\n",
        "        )\n",
        "\n",
        "        # Plotting the results\n",
        "        clear_output(wait=True)\n",
        "        plt.figure(figsize=(20, 5))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(episode_rewards)\n",
        "        plt.title(\"Episode Reward\")\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Reward\")\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(actor_losses)\n",
        "        plt.title(\"Actor Loss\")\n",
        "        plt.xlabel(\"Step\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(critic_losses)\n",
        "        plt.title(\"Critic Loss\")\n",
        "        plt.xlabel(\"Step\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tANDqHAbClTh",
        "outputId": "356df951-87cb-456a-f597-82181fc82e9e"
      },
      "outputs": [],
      "source": [
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "# let's hope this will work\n",
        "# don't forget to pray\n",
        "with gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\") as env, RecordVideo(\n",
        "    env=env, video_folder=\"./videos\"\n",
        ") as env_monitor:\n",
        "    evaluate(env_monitor, actor, low, high, n_games=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "d0XQLup0C6VC",
        "outputId": "1b6f97c6-48f3-4522-f050-8dde836f8af9"
      },
      "outputs": [],
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "import sys\n",
        "\n",
        "video_paths = sorted([s for s in Path(\"videos\").iterdir() if s.suffix == \".mp4\"])\n",
        "video_path = video_paths[-1]  # You can also try other indices\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # https://stackoverflow.com/a/57378660/1214547\n",
        "    with video_path.open(\"rb\") as fp:\n",
        "        mp4 = fp.read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "else:\n",
        "    data_url = str(video_path)\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(\n",
        "        data_url\n",
        "    )\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
