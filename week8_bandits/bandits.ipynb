{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17sK8azRkL9D",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "    !pip -q install gymnasium\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGLqKBc8kL9F"
      },
      "outputs": [],
      "source": [
        "from abc import ABCMeta, abstractmethod, abstractproperty\n",
        "import enum\n",
        "\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=3)\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set()\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ETKNBmAkL9H"
      },
      "source": [
        "## Bernoulli Bandit\n",
        "\n",
        "We are going to implement several exploration strategies for simplest problem - bernoulli bandit.\n",
        "\n",
        "The bandit has $K$ actions. Action produce 1.0 reward $r$ with probability $0 \\le \\theta_k \\le 1$ which is unknown to agent, but fixed over time. Agent's objective is to minimize regret over fixed number $T$ of action selections:\n",
        "\n",
        "$$\\rho = T\\theta^* - \\sum_{t=1}^T \\theta_{A_t}$$\n",
        "\n",
        "Where $\\theta^* = \\max_k\\{\\theta_k\\}$\n",
        "\n",
        "**Real-world analogy:**\n",
        "\n",
        "Clinical trials - we have $K$ pills and $T$ ill patient. After taking pill, patient is cured with probability $\\theta_k$. Task is to find most efficient pill.\n",
        "\n",
        "A research on clinical trials - https://arxiv.org/pdf/1507.08025.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7pHt21SkL9H"
      },
      "outputs": [],
      "source": [
        "class BernoulliBandit:\n",
        "    def __init__(self, n_actions=5):\n",
        "        self._probs = np.random.random(n_actions)\n",
        "\n",
        "    @property\n",
        "    def action_count(self):\n",
        "        return len(self._probs)\n",
        "\n",
        "    def pull(self, action):\n",
        "        if np.any(np.random.random() > self._probs[action]):\n",
        "            return 0.0\n",
        "        return 1.0\n",
        "\n",
        "    def optimal_reward(self):\n",
        "        \"\"\"Used for regret calculation\"\"\"\n",
        "        return np.max(self._probs)\n",
        "\n",
        "    def action_value(self, action):\n",
        "        \"\"\"Used for regret calculation\"\"\"\n",
        "        return self._probs[action]\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Used in nonstationary version\"\"\"\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Used in nonstationary version\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-5GHqwJkL9I"
      },
      "outputs": [],
      "source": [
        "class AbstractAgent(metaclass=ABCMeta):\n",
        "    def init_actions(self, n_actions):\n",
        "        self._successes = np.zeros(n_actions)\n",
        "        self._failures = np.zeros(n_actions)\n",
        "        self._total_pulls = 0\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_action(self):\n",
        "        \"\"\"\n",
        "        Get current best action\n",
        "        :rtype: int\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        \"\"\"\n",
        "        Observe reward from action and update agent's internal parameters\n",
        "        :type action: int\n",
        "        :type reward: int\n",
        "        \"\"\"\n",
        "        self._total_pulls += 1\n",
        "        if reward == 1:\n",
        "            self._successes[action] += 1\n",
        "        else:\n",
        "            self._failures[action] += 1\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return self.__class__.__name__\n",
        "\n",
        "\n",
        "class RandomAgent(AbstractAgent):\n",
        "    def get_action(self):\n",
        "        return #YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrBI6jHCkL9I"
      },
      "source": [
        "### Epsilon-greedy agent\n",
        "\n",
        "**for** $t = 1,2,...$ **do**\n",
        "\n",
        "&nbsp;&nbsp; **for** $k = 1,...,K$ **do**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\hat\\theta_k \\leftarrow \\alpha_k / (\\alpha_k + \\beta_k)$\n",
        "\n",
        "&nbsp;&nbsp; **end for**\n",
        "\n",
        "&nbsp;&nbsp; $x_t \\leftarrow argmax_{k}\\hat\\theta$ with probability $1 - \\epsilon$ or random action with probability $\\epsilon$\n",
        "\n",
        "&nbsp;&nbsp; Apply $x_t$ and observe $r_t$\n",
        "\n",
        "&nbsp;&nbsp; $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t}) + (r_t, 1-r_t)$\n",
        "\n",
        "**end for**\n",
        "\n",
        "Implement the algorithm above in the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1cu2JLlkL9I"
      },
      "outputs": [],
      "source": [
        "class EpsilonGreedyAgent(AbstractAgent):\n",
        "    def __init__(self, epsilon=0.01):\n",
        "        self._epsilon = epsilon\n",
        "\n",
        "    def get_action(self):\n",
        "        if np.random.random() < self._epsilon:\n",
        "            return #YOUR CODE\n",
        "        else:\n",
        "            return #YOUR CODE\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return self.__class__.__name__ + \"(epsilon={})\".format(self._epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcAasgQskL9I"
      },
      "source": [
        "### UCB Agent\n",
        "Epsilon-greedy strategy have no preference for actions. It would be better to select among actions that are uncertain or have potential to be optimal. One can come up with idea of index for each action that represents optimality and uncertainty at the same time. One efficient way to do it is to use UCB1 algorithm:\n",
        "\n",
        "**for** $t = 1,2,...$ **do**\n",
        "\n",
        "&nbsp;&nbsp; **for** $k = 1,...,K$ **do**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $w_k \\leftarrow \\alpha_k / (\\alpha_k + \\beta_k) + \\sqrt{2\\log(t) \\ / \\ (\\alpha_k + \\beta_k)}$\n",
        "\n",
        "&nbsp;&nbsp; **end for**\n",
        "\n",
        "&nbsp;&nbsp; **end for**\n",
        " $x_t \\leftarrow argmax_{k}w$\n",
        "\n",
        "&nbsp;&nbsp; Apply $x_t$ and observe $r_t$\n",
        "\n",
        "&nbsp;&nbsp; $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t}) + (r_t, 1-r_t)$\n",
        "\n",
        "**end for**\n",
        "\n",
        "__Note:__ in practice, one can multiply $\\sqrt{2\\log(t) \\ / \\ (\\alpha_k + \\beta_k)}$ by some tunable parameter to regulate agent's optimism and wilingness to abandon non-promising actions.\n",
        "\n",
        "More versions and optimality analysis - https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B11aZtF2kL9J"
      },
      "outputs": [],
      "source": [
        "class UCBAgent(AbstractAgent):\n",
        "    def get_action(self):\n",
        "        mean_reward = #YOUR CODE\n",
        "        numerator = #YOUR CODE\n",
        "        denominator = #YOUR CODE\n",
        "        return np.argmax(\n",
        "            mean_reward + np.sqrt(2 * numerator / denominator)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbcm5lc9kL9J"
      },
      "source": [
        "### Thompson sampling\n",
        "\n",
        "UCB1 algorithm does not take into account actual distribution of rewards. If we know the distribution - we can do much better by using Thompson sampling:\n",
        "\n",
        "**for** $t = 1,2,...$ **do**\n",
        "\n",
        "&nbsp;&nbsp; **for** $k = 1,...,K$ **do**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sample $\\hat\\theta_k \\sim beta(\\alpha_k, \\beta_k)$\n",
        "\n",
        "&nbsp;&nbsp; **end for**\n",
        "\n",
        "&nbsp;&nbsp; $x_t \\leftarrow argmax_{k}\\hat\\theta$\n",
        "\n",
        "&nbsp;&nbsp; Apply $x_t$ and observe $r_t$\n",
        "\n",
        "&nbsp;&nbsp; $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t}) + (r_t, 1-r_t)$\n",
        "\n",
        "**end for**\n",
        "\n",
        "\n",
        "More on Thompson Sampling:\n",
        "https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCdG757hkL9J"
      },
      "outputs": [],
      "source": [
        "class ThompsonSamplingAgent(AbstractAgent):\n",
        "    def get_action(self):\n",
        "        return #YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xU3HhsVpkL9J"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "def get_regret(env, agents, n_steps=5000, n_trials=50):\n",
        "    scores = OrderedDict(\n",
        "        {agent.name: [0.0 for step in range(n_steps)] for agent in agents}\n",
        "    )\n",
        "\n",
        "    for trial in range(n_trials):\n",
        "        env.reset()\n",
        "\n",
        "        for a in agents:\n",
        "            a.init_actions(env.action_count)\n",
        "\n",
        "        for i in range(n_steps):\n",
        "            optimal_reward = env.optimal_reward()\n",
        "\n",
        "            for agent in agents:\n",
        "                action = agent.get_action()\n",
        "                reward = env.pull(action)\n",
        "                agent.update(action, reward)\n",
        "                scores[agent.name][i] += optimal_reward - env.action_value(action)\n",
        "\n",
        "            env.step()  # change bandit's state if it is unstationary\n",
        "\n",
        "    for agent in agents:\n",
        "        scores[agent.name] = np.cumsum(scores[agent.name]) / n_trials\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "def plot_regret(agents, scores):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for agent in agents:\n",
        "        plt.plot(scores[agent.name])\n",
        "    plt.legend([agent.name for agent in agents])\n",
        "\n",
        "    plt.ylabel(\"regret\")\n",
        "    plt.xlabel(\"steps\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "KKTQu17zkL9J",
        "outputId": "2bb4db04-1b62-4b2b-d4a1-e1bf6fff63a4"
      },
      "outputs": [],
      "source": [
        "# Uncomment agents\n",
        "agents = [RandomAgent(), EpsilonGreedyAgent(), UCBAgent(), ThompsonSamplingAgent()]\n",
        "\n",
        "regret = get_regret(BernoulliBandit(), agents, n_steps=1000, n_trials=1)\n",
        "plot_regret(agents, regret)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEMbvLGJ2I8Q"
      },
      "source": [
        "# Contextual Bandits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8FbeLY5-dju"
      },
      "source": [
        "## Linear UCB\n",
        "\n",
        "The LinearUCB algorithm involves the following steps:\n",
        "$\\hat{r}_{t,a} = x_{t,a}^T \\theta_a + \\alpha \\sqrt{x_{t,a}^T A_a^{-1} x_{t,a}}$\n",
        "\n",
        "1. **Initialization**: For each action $a$, initialize:\n",
        "   - $A_a = I_d$, the identity matrix.\n",
        "   - $b_a = 0$, the zero vector.\n",
        "2. **Loop for each round $t$**:\n",
        "   - **Contextual Features**: Receive $x_{t,a}$ for each action $a$.\n",
        "   - **Prediction and Selection**: Calculate for each action $a$: $\\hat{r}_{t,a} = x_{t,a}^T \\theta_a + \\alpha \\sqrt{x_{t,a}^T A_a^{-1} x_{t,a}}$ where $\\theta_a = A_a^{-1} b_a$. Select action $a_t$ with the highest $\\hat{r}_{t,a}$.\n",
        "   - **Reward**: Observe reward $r_{t,a_t}$ for chosen action.\n",
        "   - **Update**: Update $A_{a_t}$ and $b_{a_t}$ as follows: $A_{a_t} = A_{a_t} + x_{t,a_t} x_{t,a_t}^T, b_{a_t} = b_{a_t} + r_{t,a_t} x_{t,a_t}$\n",
        "\n",
        "This method dynamically adjusts between exploring new actions and exploiting known rewards, using the context of decisions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tku6AjDPMEzg"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "\n",
        "class LinearContextualBanditEnv(gym.Env):\n",
        "\n",
        "    def __init__(self, n_actions, n_contexts, context_dim):\n",
        "        super().__init__()\n",
        "        self.n_actions = n_actions\n",
        "        self.n_contexts = n_contexts\n",
        "        self.context_dim = context_dim\n",
        "\n",
        "        self.contexts = np.random.randn(n_contexts, context_dim)\n",
        "        self.theta = np.random.randn(context_dim, n_actions)\n",
        "        self.reward_mean = self.contexts @ self.theta\n",
        "\n",
        "        self.action_space = spaces.Discrete(n_actions)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(context_dim,), dtype=np.float32\n",
        "        )\n",
        "        self.context_index = None\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        # Generate a new context randomly\n",
        "        self.context_index = np.random.choice(self.n_contexts)\n",
        "        return self.contexts[self.context_index], {}\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self.action_space.contains(action), action\n",
        "\n",
        "        # Define reward logic based on the current context and chosen action\n",
        "        self.context_index = np.random.choice(self.n_contexts)\n",
        "        reward = self._calculate_reward(action, self.context_index)\n",
        "        return self.contexts[self.context_index], reward, True, False, {}\n",
        "\n",
        "    def get_context_index(self):\n",
        "        return self.context_index\n",
        "\n",
        "    def _calculate_reward(self, action, context_index):\n",
        "        # Implement the logic to calculate the reward based on the action and context\n",
        "        reward = self.reward_mean[context_index, action] + np.random.randn()\n",
        "        return reward\n",
        "\n",
        "    def render(self):\n",
        "        # Optional: Implement rendering for human-readable output\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjGjwQuweKHp"
      },
      "outputs": [],
      "source": [
        "class VanillaUCB:\n",
        "    def __init__(self, n_contexts, n_actions):\n",
        "        self.n_actions = n_actions\n",
        "        self.n_contexts = n_contexts\n",
        "\n",
        "        self.counts = np.zeros((self.n_contexts, n_actions))  # Count of actions taken\n",
        "        self.values = np.zeros(\n",
        "            (self.n_contexts, n_actions)\n",
        "        )  # Estimated values of actions\n",
        "        self.total_count = np.zeros(self.n_contexts)  # Total count of actions taken\n",
        "\n",
        "    def get_action(self, context_index):\n",
        "        numerator = #YOUR CODE\n",
        "        denominator = #YOUR CODE\n",
        "        upper_bound = self.values[context_index] + np.sqrt(2 * numerator / denominator)\n",
        "        action = np.argmax(upper_bound)\n",
        "        self.total_count[context_index] += 1\n",
        "        return action\n",
        "\n",
        "    def update(self, context_index, action, reward):\n",
        "        self.counts[context_index, action] += 1\n",
        "        n = self.counts[context_index, action]\n",
        "        value = self.values[context_index, action]\n",
        "        new_value = #YOUR CODE\n",
        "        self.values[context_index, action] = new_value\n",
        "\n",
        "\n",
        "class LinearUCB:\n",
        "    def __init__(self, alpha, context_dim, n_actions):\n",
        "        self.alpha = alpha  # exploration parameter\n",
        "        self.context_dim = context_dim  # number of features\n",
        "        self.n_actions = n_actions\n",
        "        self.A = np.array(\n",
        "            [np.identity(self.context_dim) for _ in range(self.n_actions)]\n",
        "        )\n",
        "        self.A_inv = self.A.copy()\n",
        "        self.b = np.array([np.zeros(self.context_dim) for _ in range(self.n_actions)])\n",
        "\n",
        "    def get_action(self, context):\n",
        "        theta = #YOUR CODE\n",
        "        p = #YOUR CODE\n",
        "        return np.argmax(p)\n",
        "\n",
        "    def update(self, context, action, reward):\n",
        "        self.A[action] += #YOUR CODE\n",
        "        self.A_inv[action] = np.linalg.inv(self.A[action])\n",
        "        self.b[action] += #YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "a-Lb1UQS-gxq",
        "outputId": "f61498a1-d411-47f0-93e5-ba2e90ea2ff0"
      },
      "outputs": [],
      "source": [
        "T = 1000\n",
        "n_actions = 2\n",
        "n_contexts = 5\n",
        "context_dim = 4\n",
        "alpha = 1.0  # exploration parameter\n",
        "\n",
        "env = LinearContextualBanditEnv(n_actions, n_contexts, context_dim)\n",
        "linear_ucb = LinearUCB(alpha, context_dim, n_actions)\n",
        "vanilla_ucb = VanillaUCB(n_contexts, n_actions)\n",
        "\n",
        "linear_regrets = [0]\n",
        "vanilla_regrets = [0]\n",
        "\n",
        "context, _ = env.reset()\n",
        "\n",
        "# VanillaUCB\n",
        "for t in range(T):\n",
        "    action = linear_ucb.get_action(context)\n",
        "    reward_mean = env.reward_mean[env.context_index]\n",
        "\n",
        "    next_context, reward, _, _, _ = env.step(action)\n",
        "    linear_ucb.update(context, action, reward)\n",
        "    context = next_context\n",
        "    linear_regrets.append(linear_regrets[-1] + reward_mean.max() - reward_mean[action])\n",
        "\n",
        "env.reset()\n",
        "context_index = env.get_context_index()\n",
        "# LinearUCB\n",
        "for t in range(T):\n",
        "\n",
        "    action = vanilla_ucb.get_action(context_index)\n",
        "    reward_mean = env.reward_mean[env.context_index]\n",
        "\n",
        "    _, reward, _, _, _ = env.step(action)\n",
        "    vanilla_ucb.update(context_index, action, reward)\n",
        "    context_index = env.get_context_index()\n",
        "    vanilla_regrets.append(\n",
        "        vanilla_regrets[-1] + reward_mean.max() - reward_mean[action]\n",
        "    )\n",
        "\n",
        "plt.plot(linear_regrets[1:], label=\"LinearUCB\")\n",
        "plt.plot(vanilla_regrets[1:], label=\"VanillaUCB\")\n",
        "plt.legend()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
