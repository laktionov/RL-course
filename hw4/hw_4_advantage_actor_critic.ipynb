{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ru_4OlKkRJs"
      },
      "source": [
        "# Implementing Advantage-Actor Critic (A2C) - 4 pts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSDb2Y9XkRJu"
      },
      "source": [
        "In this notebook you will implement Advantage Actor Critic algorithm that trains on a batch of Atari 2600 environments running in parallel. \n",
        "\n",
        "Firstly, we will use environment wrappers implemented in file `atari_wrappers.py`. These wrappers preprocess observations (resize, grayscal, take max between frames, skip frames, stack them together, prepares for PyTorch and normalizes to [0, 1]) and rewards. Some of the wrappers help to reset the environment and pass `done` flag equal to `True` when agent dies.\n",
        "File `env_batch.py` includes implementation of `ParallelEnvBatch` class that allows to run multiple environments in parallel. To create an environment we can use `nature_dqn_env` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzOj1ZlisBkg"
      },
      "outputs": [],
      "source": [
        "!pip install gym==0.25.2\n",
        "!pip install -U gym[atari,accept-rom-license]\n",
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%load_ext tensorboard\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:43:34.476855Z",
          "start_time": "2020-09-11T09:43:32.696812Z"
        },
        "id": "6RsSY8ejkRJv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from atari_wrappers import nature_dqn_env\n",
        "\n",
        "nenvs = 8    # change this if you have more than 8 CPU ;)\n",
        "\n",
        "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=nenvs)\n",
        "\n",
        "n_actions = env.action_space.spaces[0].n\n",
        "obs = env.reset()\n",
        "assert obs.shape == (nenvs, 4, 84, 84)\n",
        "assert obs.dtype == np.float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwGObAsekRJv"
      },
      "source": [
        "Next, we will need to implement a model that predicts logits of policy distribution and critic value. Use shared backbone. You may use same architecture as in DQN task with one modification: instead of having a single output layer, it must have two output layers taking as input the output of the last hidden layer (one for actor, one for critic). \n",
        "\n",
        "Still it may be very helpful to make more changes:\n",
        "* use orthogonal initialization with gain $\\sqrt{2}$ and initialize biases with zeros;\n",
        "* use more filters (e.g. 32-64-64 instead of 16-32-64);\n",
        "* use two-layer heads for actor and critic or add a linear layer into backbone;\n",
        "\n",
        "**Danger:** do not divide on 255, input is already normalized to [0, 1] in our wrappers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:15.493948Z",
          "start_time": "2020-09-11T05:45:15.065561Z"
        },
        "id": "6nhH1MopkRJv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class A2CNetwork(nn.Module):\n",
        "    # <Define your model here>\n",
        "\n",
        "'''\n",
        "input:\n",
        "    states - tensor, (batch_size x channels x width x height)\n",
        "output:\n",
        "    logits - tensor, logits of action probabilities for your actor policy, (batch_size x num_actions)\n",
        "    V - tensor, critic estimation, (batch_size)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EpyEfjHkRJw"
      },
      "source": [
        "You will also need to define and use a policy that wraps the model. While the model computes logits for all actions, the policy will sample actions and also compute their log probabilities.  `policy.act` should return a **dictionary** of all the arrays that are needed to interact with an environment and train the model.\n",
        "\n",
        "**Important**: \"actions\" will be sent to environment, they must be numpy array or list, not PyTorch tensor.\n",
        "\n",
        "Note: you can add more keys, e.g. it can be convenient to compute entropy right here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:15.508916Z",
          "start_time": "2020-09-11T05:45:15.494964Z"
        },
        "id": "ey5W80QckRJw"
      },
      "outputs": [],
      "source": [
        "from torch.distributions import Categorical\n",
        "\n",
        "class Policy:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def act(self, inputs):\n",
        "        '''\n",
        "        input:\n",
        "            inputs - numpy array, (batch_size x channels x width x height)\n",
        "        output: dict containing keys ['actions', 'logits', 'log_probs', 'values']:\n",
        "            'actions' - selected actions, numpy, (batch_size)\n",
        "            'logits' - actions logits, tensor, (batch_size x num_actions)\n",
        "            'log_probs' - log probs of selected actions, tensor, (batch_size)\n",
        "            'values' - critic estimations, tensor, (batch_size)\n",
        "        '''\n",
        "        <YOUR CODE>\n",
        "        \n",
        "        return {\n",
        "            \"actions\": <>,\n",
        "            \"logits\": <>,\n",
        "            \"log_probs\": <>,\n",
        "            \"values\": <>,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4YaKtT1kRJw"
      },
      "source": [
        "Next we will pass the environment and policy to a runner that collects rollouts from the environment. \n",
        "The class is already implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:15.635649Z",
          "start_time": "2020-09-11T05:45:15.510915Z"
        },
        "id": "I3Czcvt7kRJx"
      },
      "outputs": [],
      "source": [
        "from runners import EnvRunner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UoZPBCHkRJx"
      },
      "source": [
        "This runner interacts with the environment for a given number of steps and returns a dictionary containing\n",
        "keys \n",
        "\n",
        "* 'observations' \n",
        "* 'rewards' \n",
        "* 'dones'\n",
        "* 'actions'\n",
        "* all other keys that you defined in `Policy`\n",
        "\n",
        "under each of these keys there is a python `list` of interactions with the environment of specified length $T$ &mdash; the size of partial trajectory, or rollout length. Let's have a look at how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:17.267442Z",
          "start_time": "2020-09-11T05:45:15.636676Z"
        },
        "id": "WvP1M-8gkRJx"
      },
      "outputs": [],
      "source": [
        "model = <init your model>\n",
        "policy = Policy(model)\n",
        "runner = EnvRunner(env, policy, nsteps=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:18.244263Z",
          "start_time": "2020-09-11T05:45:17.268410Z"
        },
        "id": "rgek3SOJkRJx"
      },
      "outputs": [],
      "source": [
        "# generates new rollout\n",
        "trajectory = runner.get_next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:18.260254Z",
          "start_time": "2020-09-11T05:45:18.245263Z"
        },
        "id": "X8I8NqJYkRJx"
      },
      "outputs": [],
      "source": [
        "# what is inside\n",
        "print(trajectory.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:18.418163Z",
          "start_time": "2020-09-11T05:45:18.262253Z"
        },
        "id": "Cleh8T7_kRJx"
      },
      "outputs": [],
      "source": [
        "# Sanity checks\n",
        "assert 'logits' in trajectory, \"Not found: policy didn't provide logits\"\n",
        "assert 'log_probs' in trajectory, \"Not found: policy didn't provide log_probs of selected actions\"\n",
        "assert 'values' in trajectory, \"Not found: policy didn't provide critic estimations\"\n",
        "assert trajectory['logits'][0].shape == (nenvs, n_actions), \"logits wrong shape\"\n",
        "assert trajectory['log_probs'][0].shape == (nenvs,), \"log_probs wrong shape\"\n",
        "assert trajectory['values'][0].shape == (nenvs,), \"values wrong shape\"\n",
        "\n",
        "for key in trajectory.keys():\n",
        "    assert len(trajectory[key]) == 5, \\\n",
        "    f\"something went wrong: 5 steps should have been done, got trajectory of length {len(trajectory[key])} for '{key}'\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDKUeUlfkRJx"
      },
      "source": [
        "Now let's work with this trajectory a bit. To train the critic you will need to compute the value targets. It will also be used as an estimation of $Q$ for actor training.\n",
        "\n",
        "You should use all available rewards for value targets, so the formula for the value targets is simple:\n",
        "\n",
        "$$\n",
        "\\hat v(s_t) = \\sum_{t'=0}^{T - 1}\\gamma^{t'}r_{t+t'} + \\gamma^T \\hat{v}(s_{t+T}),\n",
        "$$\n",
        "\n",
        "where $s_{t + T}$ is the latest observation of the environment.\n",
        "\n",
        "Any callable could be passed to `EnvRunner` to be applied to each partial trajectory after it is collected. \n",
        "Thus, we can implement and use `ComputeValueTargets` callable. \n",
        "\n",
        "**Do not forget** to use `trajectory['dones']` flags to check if you need to add the value targets at the next step when \n",
        "computing value targets for the current step.\n",
        "\n",
        "**Bonus (+1 pt):** implement [Generalized Advantage Estimation (GAE)](https://arxiv.org/pdf/1506.02438.pdf) instead; use $\\lambda \\approx 0.95$ or even closer to 1 in experiment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T10:29:23.189515Z",
          "start_time": "2020-09-11T10:29:23.173510Z"
        },
        "id": "RshQcHgPkRJy"
      },
      "outputs": [],
      "source": [
        "class ComputeValueTargets:\n",
        "    def __init__(self, policy, gamma=0.99):\n",
        "        self.policy = policy\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def __call__(self, trajectory, latest_observation):\n",
        "        '''\n",
        "        This method should modify trajectory inplace by adding \n",
        "        an item with key 'value_targets' to it\n",
        "        \n",
        "        input:\n",
        "            trajectory - dict from runner\n",
        "            latest_observation - last state, numpy, (num_envs x channels x width x height)\n",
        "        '''\n",
        "        <YOUR CODE>\n",
        "        \n",
        "        trajectory['value_targets'] = <>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDrCV9XEkRJy"
      },
      "source": [
        "After computing value targets we will transform lists of interactions into tensors\n",
        "with the first dimension `batch_size` which is equal to `T * nenvs`.\n",
        "\n",
        "You need to make sure that after this transformation `\"log_probs\"`, `\"value_targets\"`, `\"values\"` are 1-dimensional PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:18.623944Z",
          "start_time": "2020-09-11T05:45:18.515153Z"
        },
        "id": "noKYEi4ikRJy"
      },
      "outputs": [],
      "source": [
        "class MergeTimeBatch:\n",
        "    \"\"\" Merges first two axes typically representing time and env batch. \"\"\"\n",
        "    def __call__(self, trajectory, latest_observation):\n",
        "        # Modify trajectory inplace. \n",
        "        <YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXirNkSDkRJy"
      },
      "source": [
        "Let's do more sanity checks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:18.782710Z",
          "start_time": "2020-09-11T05:45:18.624949Z"
        },
        "id": "zSATsAIlkRJy"
      },
      "outputs": [],
      "source": [
        "runner = EnvRunner(env, policy, nsteps=5, transforms=[ComputeValueTargets(policy),\n",
        "                                                      MergeTimeBatch()])\n",
        "\n",
        "trajectory = runner.get_next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:18.878653Z",
          "start_time": "2020-09-11T05:45:18.784709Z"
        },
        "id": "q7F9O4_HkRJy"
      },
      "outputs": [],
      "source": [
        "# More sanity checks\n",
        "assert 'value_targets' in trajectory, \"Value targets not found\"\n",
        "assert trajectory['log_probs'].shape == (5 * nenvs,)\n",
        "assert trajectory['value_targets'].shape == (5 * nenvs,)\n",
        "assert trajectory['values'].shape == (5 * nenvs,)\n",
        "\n",
        "assert trajectory['log_probs'].requires_grad, \"Gradients are not available for actor head!\"\n",
        "assert trajectory['values'].requires_grad, \"Gradients are not available for critic head!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMTYNSe_kRJy"
      },
      "source": [
        "Now is the time to implement the advantage actor critic algorithm itself. You can look into [Mnih et al. 2016](https://arxiv.org/abs/1602.01783) paper, and lectures ([part 1](https://www.youtube.com/watch?v=Ds1trXd6pos&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=5), [part 2](https://www.youtube.com/watch?v=EKqxumCuAAY&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=6)) by Sergey Levine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Actor-critic objective\n",
        "\n",
        "Here we define a loss function that uses rollout above to train advantage actor-critic agent.\n",
        "\n",
        "\n",
        "Our loss consists of three components:\n",
        "\n",
        "* __The policy \"loss\"__\n",
        " $$ \\hat J = {1 \\over T} \\cdot \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s,a) $$\n",
        "  * This function has no meaning in and of itself, but it was built such that\n",
        "  * $ \\nabla \\hat J = {1 \\over N} \\cdot \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s,a) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n",
        "  * Therefore if we __maximize__ J_hat with gradient descent we will maximize expected reward\n",
        "  \n",
        "  \n",
        "* __The value \"loss\"__\n",
        "  $$ L_{td} = {1 \\over T} \\cdot \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n",
        "  * Ye Olde TD_loss from q-learning and alike\n",
        "  * If we minimize this loss, V(s) will converge to $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n",
        "\n",
        "\n",
        "* __Entropy Regularizer__\n",
        "  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n",
        "  * If we __maximize__ entropy we discourage agent from predicting zero probability to actions\n",
        "  prematurely (a.k.a. exploration)\n",
        "  \n",
        "  \n",
        "So we optimize a linear combination of $L_{td}$ $- \\hat J$, $-H$\n",
        "  \n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "__One more thing:__ since we train on T-step rollouts, we can use N-step formula for advantage for free:\n",
        "  * At the last step, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s_t) $\n",
        "  * One step earlier, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s_t) $\n",
        "  * Et cetera, et cetera. This way agent starts training much faster since it's estimate of A(s,a) depends less on his (imperfect) value function and more on actual rewards. There's also a [nice generalization](https://arxiv.org/abs/1506.02438) of this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:18.990212Z",
          "start_time": "2020-09-11T05:45:18.880653Z"
        },
        "id": "X4Sn1j8skRJy"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "class A2C:\n",
        "    def __init__(self, policy, optimizer, value_loss_coef=0.25, entropy_coef=0.01, max_grad_norm=0.5):\n",
        "        self.policy = policy\n",
        "        self.optimizer = optimizer\n",
        "        self.value_loss_coef = value_loss_coef\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "    \n",
        "    def loss(self, trajectory, write):\n",
        "        # compute all losses\n",
        "        # do not forget to use weights for critic loss and entropy loss\n",
        "        <YOUR CODE>\n",
        "        \n",
        "        # log all losses\n",
        "        write('losses', {\n",
        "            'policy loss': <>,\n",
        "            'critic loss': <>,\n",
        "            'entropy loss': <>\n",
        "        })\n",
        "        \n",
        "        # additional logs\n",
        "        write('critic/advantage', <>)\n",
        "        write('critic/values', {\n",
        "            'value predictions': <>,\n",
        "            'value targets':     <>,\n",
        "        })\n",
        "        \n",
        "        # return scalar loss\n",
        "        return <>               \n",
        "\n",
        "    def train(self, runner):\n",
        "        # collect trajectory using runner\n",
        "        # compute loss and perform one step of gradient optimization\n",
        "        # do not forget to clip gradients\n",
        "        <YOUR CODE>\n",
        "        \n",
        "        # use runner.write to log scalar to tensorboard\n",
        "        runner.write('gradient norm', <>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLEW45ixkRJz"
      },
      "source": [
        "Now you can train your model. For optimization we suggest you use RMSProp with learning rate 7e-4 (you can also linearly decay it to 0), smoothing constant (alpha in PyTorch) equal to 0.99 and epsilon equal to 1e-5.\n",
        "\n",
        "We recommend to train for at least 10 million environment steps across all batched environments (takes ~3 hours on a single GTX1080 with 8 CPU). It should be possible to achieve *average raw reward over last 100 episodes* (the average is taken over 100 last episodes in each environment in the batch) of about 600. **Your goal is to reach 500**.\n",
        "\n",
        "Notes:\n",
        "* if your reward is stuck at ~200 for more than 2M steps then probably there is a bug\n",
        "* if your gradient norm is >10 something probably went wrong\n",
        "* make sure your `entropy loss` is negative, your `critic loss` is positive\n",
        "* make sure you didn't forget `.detach` in losses where it's needed\n",
        "* `actor loss` should oscillate around zero or near it; do not expect loss to decrease in RL ;)\n",
        "* you can experiment with `nsteps` (\"rollout length\"); standard rollout length is 5 or 10. Note that this parameter influences how many algorithm iterations is required to train on 10M steps (or 40M frames --- we used frameskip in preprocessing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:19.196652Z",
          "start_time": "2020-09-11T05:45:18.991227Z"
        },
        "id": "p2uo3WfQkRJz"
      },
      "outputs": [],
      "source": [
        "model = <init your model>\n",
        "policy = Policy(model)\n",
        "runner = EnvRunner(env, policy, nsteps=10, transforms=[ComputeValueTargets(policy),\n",
        "                                                      MergeTimeBatch()])\n",
        "\n",
        "optimizer = <choose your fighter>\n",
        "\n",
        "a2c = <init A2C algorithm>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:03:19.815094Z",
          "start_time": "2020-09-11T05:45:19.278605Z"
        },
        "id": "V-gVjNvAkRJz"
      },
      "outputs": [],
      "source": [
        "<YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:03:19.847076Z",
          "start_time": "2020-09-11T09:03:19.816094Z"
        },
        "id": "l7s42BFokRJz"
      },
      "outputs": [],
      "source": [
        "# save your model just in case \n",
        "torch.save(model.state_dict(), \"A2C\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:03:20.070164Z",
          "start_time": "2020-09-11T09:03:19.848077Z"
        },
        "id": "KaBLt8ozkRJz"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7mzN0BukRJz"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:03:20.292111Z",
          "start_time": "2020-09-11T09:03:20.071165Z"
        },
        "id": "rd4KkuRjkRJz"
      },
      "outputs": [],
      "source": [
        "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=None, \n",
        "                     clip_reward=False, summaries=False, episodic_life=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:03:20.307488Z",
          "start_time": "2020-09-11T09:03:20.293112Z"
        },
        "id": "lncShpeRkRJz"
      },
      "outputs": [],
      "source": [
        "def evaluate(env, policy, n_games=1, t_max=10000):\n",
        "    '''\n",
        "    Plays n_games and returns rewards\n",
        "    '''\n",
        "    rewards = []\n",
        "    \n",
        "    for _ in range(n_games):\n",
        "        s = env.reset()\n",
        "        \n",
        "        R = 0\n",
        "        for _ in range(t_max):\n",
        "            action = policy.act(np.array([s]))[\"actions\"][0]\n",
        "            \n",
        "            s, r, done, _ = env.step(action)\n",
        "            \n",
        "            R += r\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        rewards.append(R)\n",
        "    return np.array(rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:26:18.357449Z",
          "start_time": "2020-09-11T09:25:35.258389Z"
        },
        "id": "0an5jDvEkRJz"
      },
      "outputs": [],
      "source": [
        "# evaluation will take some time!\n",
        "sessions = evaluate(env, policy, n_games=30)\n",
        "score = sessions.mean()\n",
        "print(f\"Your score: {score}\")\n",
        "\n",
        "assert score >= 500, \"Needs more training?\"\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:26:18.373428Z",
          "start_time": "2020-09-11T09:26:18.359434Z"
        },
        "id": "oEvWYpY7kRJz"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUjT3uFnkRJ0"
      },
      "source": [
        "## Record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:26:18.657314Z",
          "start_time": "2020-09-11T09:26:18.375172Z"
        },
        "id": "iKsBwdwskRJ0"
      },
      "outputs": [],
      "source": [
        "env_monitor = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=None, monitor=True,\n",
        "                             clip_reward=False, summaries=False, episodic_life=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:38:24.509378Z",
          "start_time": "2020-09-11T09:38:16.392351Z"
        },
        "id": "GSfN0bdokRJ0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# record sessions\n",
        "sessions = evaluate(env_monitor, policy, n_games=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:38:51.559623Z",
          "start_time": "2020-09-11T09:38:51.521317Z"
        },
        "id": "od4XjKhPkRJ0"
      },
      "outputs": [],
      "source": [
        "# rewards for recorded games\n",
        "sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:26:18.373428Z",
          "start_time": "2020-09-11T09:26:18.359434Z"
        },
        "id": "DkvqNvG1kRJ0"
      },
      "outputs": [],
      "source": [
        "env_monitor.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
