{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j9dfWABRNCVm",
    "outputId": "f6c96e12-6707-4b66-c33b-63a0a6a6c16f"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "\n",
    "    !pip -q install gymnasium[mujoco]==1.0.0\n",
    "\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI0kqjVINCVt"
   },
   "source": [
    "# HW4: PPO (up to 5 points)\n",
    "\n",
    "\n",
    "In this notebook you will be implementing Proximal Policy Optimization algorithm,\n",
    "scaled up version of which was used to train [OpenAI Five](https://openai.com/blog/openai-five/)\n",
    "to [win](https://openai.com/blog/how-to-train-your-openai-five/) against the\n",
    "world champions in Dota 2.\n",
    "You will be solving a continuous control environment on which it may be easier and faster\n",
    "to train an agent, however note that PPO here may not be the best algorithm as, for example,\n",
    "Soft Actor Critic may be more suited\n",
    "for continuous control environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZeKC7aO4RIC"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y2N76WIRNCVx",
    "outputId": "d0f12d63-6041-4afe-faca-fc15de863135"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "ENV_NAME = \"HalfCheetah-v5\"\n",
    "\n",
    "\n",
    "def make_env(**kwargs):\n",
    "    return gym.make(ENV_NAME, **kwargs)\n",
    "\n",
    "\n",
    "env = make_env(render_mode=\"rgb_array\")\n",
    "print(\"Observation space: \", env.observation_space)\n",
    "print(\"Action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "XRMAhWjN48Qx",
    "outputId": "a8ea46de-8517-4c51-c6a4-57f4c525474e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env.reset()\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9_3dfCBGolj"
   },
   "source": [
    "[Following 37 implementation details of PPO](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#Andrychowicz), we apply all necessary wrappers to the environment. We also use a vectorized env to collect transitions in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYDYzBuHEM8M"
   },
   "outputs": [],
   "source": [
    "from gymnasium.vector import AsyncVectorEnv\n",
    "from gymnasium.wrappers.vector import (\n",
    "    NormalizeObservation,\n",
    "    NormalizeReward,\n",
    "    ClipReward,\n",
    "    RecordEpisodeStatistics,\n",
    ")\n",
    "\n",
    "\n",
    "def make_vec_env(num_envs, **kwargs):\n",
    "    \"\"\"Creates a vectorized Atari environment with preprocessing.\"\"\"\n",
    "\n",
    "    # Create a list of environment initialization functions\n",
    "    env_fns = [lambda: make_env(**kwargs) for i in range(num_envs)]\n",
    "    envs = AsyncVectorEnv(env_fns, shared_memory=True)\n",
    "    envs = RecordEpisodeStatistics(envs, buffer_length=100000)\n",
    "\n",
    "    # Use running statistics to scale observation\n",
    "    # To have zero mean and unit std\n",
    "    envs = NormalizeObservation(envs)\n",
    "\n",
    "    # Use running std to scale reward\n",
    "    envs = NormalizeReward(envs, gamma=0.99)\n",
    "\n",
    "    # Clip reward after normalization\n",
    "    envs = ClipReward(envs, min_reward=-10, max_reward=10)\n",
    "\n",
    "    return envs\n",
    "\n",
    "\n",
    "# Parameters\n",
    "n_envs = 2  # Number of parallel environments\n",
    "# Create the vectorized environment\n",
    "vec_env = make_vec_env(n_envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9lf2Z8bNCV0"
   },
   "source": [
    "Next, you will need to define a model for training. We suggest that you use two separate networks: one for policy\n",
    "and another for value function. Each network should be a 3-layer MLP with 64 hidden units, $\\mathrm{tanh}$\n",
    "activation function.\n",
    "\n",
    "Our policy distribution is going to be multivariate normal with diagonal covariance: $\\mathcal{N}(\\mu_\\theta(s), diag (\\sigma_\\theta^2(s)))$\n",
    "The network from above will predict the mean, and the covariance should be represented by a single\n",
    "(learned) vector of size 6 (corresponding to the dimensionality of the action space from above). Or you can also predict the variance using your model. To construct a network akin to the one utilized in the test, the output vector should be twice the size of the action space. The first half of this vector should represent the mean of the actions, while the second half denotes the standard deviation of those actions. Additionally, apply the [softplus function](https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html) to the second half of the output vector to ensure the standard deviation values are positive.\n",
    "\n",
    "Overall the model should return three things: predicted mean and variance of the distribution and value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwwLcUApNCV0"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "class PolicyModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.h = 64\n",
    "\n",
    "        self.policy_model = #<YOUR_CODE>\n",
    "\n",
    "        self.value_model = #<YOUR_CODE>\n",
    "\n",
    "    def get_policy(self, x):\n",
    "        #<YOUR_CODE>\n",
    "        return mean, var\n",
    "\n",
    "    def get_value(self, x):\n",
    "        #<YOUR_CODE>\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        policy = self.get_policy(x)\n",
    "        value = self.get_value(x)\n",
    "\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YI6d9Xq_NCV1"
   },
   "source": [
    "This model will be wrapped by a `Policy`. The policy can work in two modes, but in either case\n",
    "it is going to return dictionary with string-type keys. The first mode is when the policy is\n",
    "used to sample actions for a trajectory which will later be used for training. In this case\n",
    "the flag `training` passed to `act` method is `False` and the method should return\n",
    "a `dict` with the following keys:\n",
    "\n",
    "* `\"actions\"`: sampled actions to pass to the environment\n",
    "* `\"log_probs\"`: log-probabilities of sampled actions\n",
    "* `\"values\"`: value function $V^\\pi(s)$ predictions.\n",
    "\n",
    "We don't need to use the values under these keys for training, so all of them should be of type `np.ndarray`.\n",
    "\n",
    "When `training` is `True`, the model is training on a given batch of observations. In this\n",
    "case it should return a `dict` with the following keys\n",
    "\n",
    "* `\"distribution\"`: an instance of multivariate normal distribution (`torch.distributions.MultivariateNormal`)\n",
    "* `\"values\"`: value function $V^\\pi(s)$ prediction.\n",
    "\n",
    "The distinction about the modes comes into play depending on where the policy is used: if it is called from `EnvRunner`,\n",
    "the `training` flag is `False`, if it is called from `PPO`, the `training` flag is `True`. These classes\n",
    "will be described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C54tGZahXbSa"
   },
   "outputs": [],
   "source": [
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def act(self, inputs, training=False):\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "        mean, var = #<YOUR_CODE>\n",
    "        cov_matrix = #<YOUR_CODE>\n",
    "        normal_distr = MultivariateNormal(mean, cov_matrix)\n",
    "\n",
    "        actions = #<YOUR_CODE>\n",
    "        log_probs = #<YOUR_CODE>\n",
    "\n",
    "        values = #<YOUR_CODE>\n",
    "\n",
    "        if training:\n",
    "            return {\"distribution\": normal_distr, \"values\": values.squeeze()}\n",
    "\n",
    "        return {\n",
    "            \"actions\": actions.cpu().numpy().squeeze(),\n",
    "            \"log_probs\": log_probs.detach().cpu().numpy().squeeze(),\n",
    "            \"values\": values.detach().cpu().numpy().squeeze(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kML-jSsNCV2"
   },
   "source": [
    "We will use `EnvRunner` to perform interactions with an environment with a policy for a fixed number of timesteps. Calling `.get_next()` on a runner will return a trajectory &mdash; dictionary\n",
    "containing keys\n",
    "\n",
    "* `\"observations\"`\n",
    "* `\"rewards\"`\n",
    "* `\"resets\"`\n",
    "* `\"actions\"`\n",
    "* all other keys that you defined in `Policy`,\n",
    "\n",
    "under each of these keys there is a `np.ndarray` of specified length $T$ &mdash; the size of partial trajectory.\n",
    "\n",
    "Additionally, before returning a trajectory this runner can apply a list of transformations.\n",
    "Each transformation is simply a callable that should modify passed trajectory in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LeiuohcNCV3"
   },
   "outputs": [],
   "source": [
    "class AsArray:\n",
    "    \"\"\"\n",
    "    Converts lists of interactions to ndarray.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, trajectory):\n",
    "        # Modify trajectory inplace.\n",
    "        for k, v in filter(lambda kv: kv[0] != \"state\", trajectory.items()):\n",
    "            trajectory[k] = np.asarray(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9pQeFux48Qy"
   },
   "outputs": [],
   "source": [
    "\"\"\" RL env runner \"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EnvRunner:\n",
    "    \"\"\"Reinforcement learning runner in an environment with given policy\"\"\"\n",
    "\n",
    "    def __init__(self, env, policy, rollout_length, transforms=None, step_var=None):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.rollout_length = rollout_length\n",
    "        self.transforms = transforms or []\n",
    "        self.step_var = step_var if step_var is not None else 0\n",
    "        self.state = {\"latest_observation\": self.env.reset()[0]}\n",
    "\n",
    "    @property\n",
    "    def num_envs(self):\n",
    "        \"\"\"Returns number of batched envs or `None` if env is not batched\"\"\"\n",
    "        return getattr(self.env, \"num_envs\", None)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Resets env and runner states.\"\"\"\n",
    "        self.state[\"latest_observation\"], info = self.env.reset(**kwargs)\n",
    "        self.policy.reset()\n",
    "\n",
    "    def get_next(self):\n",
    "        \"\"\"Runs the agent in the environment.\"\"\"\n",
    "        trajectory = defaultdict(list, {\"actions\": []})\n",
    "        observations = []\n",
    "        rewards = []\n",
    "        resets = []\n",
    "        self.state[\"env_steps\"] = self.rollout_length\n",
    "\n",
    "        for i in range(self.rollout_length):\n",
    "            observations.append(self.state[\"latest_observation\"])\n",
    "            act = self.policy.act(self.state[\"latest_observation\"])\n",
    "            if \"actions\" not in act:\n",
    "                raise ValueError(\n",
    "                    \"result of policy.act must contain 'actions' \"\n",
    "                    f\"but has keys {list(act.keys())}\"\n",
    "                )\n",
    "            for key, val in act.items():\n",
    "                trajectory[key].append(val)\n",
    "\n",
    "            obs, rew, terminated, truncated, _ = self.env.step(\n",
    "                trajectory[\"actions\"][-1]\n",
    "            )\n",
    "            done = np.logical_or(terminated, truncated)\n",
    "            self.state[\"latest_observation\"] = obs\n",
    "            rewards.append(rew)\n",
    "            resets.append(done)\n",
    "            self.step_var += self.num_envs or 1\n",
    "\n",
    "            # Only reset if the env is not batched. Batched envs should\n",
    "            # auto-reset.\n",
    "            if not self.num_envs and np.all(done):\n",
    "                self.state[\"env_steps\"] = i + 1\n",
    "                self.state[\"latest_observation\"] = self.env.reset()[0]\n",
    "\n",
    "        trajectory.update(observations=observations, rewards=rewards, resets=resets)\n",
    "        trajectory[\"state\"] = self.state\n",
    "\n",
    "        for transform in self.transforms:\n",
    "            transform(trajectory)\n",
    "        return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNzPNxKLNCV4",
    "outputId": "5f43d63a-d973-4b4d-bbe2-9baab0250011"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class DummyPolicy:\n",
    "    def act(self, inputs, training=False):\n",
    "        num_envs = inputs.shape[0]\n",
    "        assert not training\n",
    "        return {\"actions\": np.random.randn(num_envs, 6), \"values\": [np.nan] * num_envs}\n",
    "\n",
    "\n",
    "runner = EnvRunner(vec_env, DummyPolicy(), 3, transforms=[AsArray()])\n",
    "trajectory = runner.get_next()\n",
    "\n",
    "{k: v.shape for k, v in trajectory.items() if k != \"state\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyhcvAMZNCV5"
   },
   "source": [
    "You will need to implement the following two transformations.\n",
    "\n",
    "The first is `GAE` that implements [Generalized Advantage Estimator](https://arxiv.org/abs/1506.02438).\n",
    "You should add two keys to the trajectory: `\"advantages\"` and `\"value_targets\"`. In GAE the advantages\n",
    "$A_t^{\\mathrm{GAE}(\\gamma,\\lambda)}$ are essentially defined as the exponential\n",
    "moving average with parameter $\\lambda$ of the regular advantages\n",
    "$\\hat{A}^{(T)}(s_t) = \\sum_{l=0}^{T-t-1} \\gamma^l r_{t+l} + \\gamma^{T} V^\\pi(s_{T}) - V^\\pi(s_t)$.\n",
    "The exact formula for the computation is the following\n",
    "\n",
    "$$\n",
    "A_{t}^{\\mathrm{GAE}(\\gamma,\\lambda)} = \\sum_{l=0}^{T-t-1} (\\gamma\\lambda)^l\\delta_{t + l}^V, \\, t \\in [0, T)\n",
    "$$\n",
    "where $\\delta_{t+l}^V = r_{t+l} + \\gamma V^\\pi(s_{t+l+1}) - V^\\pi(s_{t+l})$. You can look at the\n",
    "derivation (formulas 11-16) in the paper. Don't forget to reset the summation on terminal\n",
    "states as determined by the flags `trajectory[\"resets\"]`. You can use `trajectory[\"values\"]`\n",
    "to get values of all observations except the most recent which is stored under\n",
    " `trajectory[\"state\"][\"latest_observation\"]`. For this observation you will need to call the policy\n",
    " to get the value prediction.\n",
    "\n",
    "Once you computed the advantages, you can get the targets for training the value function by adding\n",
    "back values:\n",
    "$$\n",
    "\\hat{V}(s_{t}) = A_{t}^{\\mathrm{GAE}(\\gamma,\\lambda)} + V(s_{t}),\n",
    "$$\n",
    "where $\\hat{V}$ is a tensor of value targets that are used to train the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AC5rUuiKNCV6"
   },
   "outputs": [],
   "source": [
    "class GAE:\n",
    "    \"\"\"Generalized Advantage Estimator.\"\"\"\n",
    "\n",
    "    def __init__(self, policy, gamma=0.99, lambda_=0.95):\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def __call__(self, trajectory):\n",
    "        gamma = self.gamma\n",
    "        lambda_ = self.lambda_\n",
    "\n",
    "        #<YOUR_CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkDCek2p-OpZ"
   },
   "source": [
    "**Test GAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBMP47qk978g",
    "outputId": "8dfe719a-cd07-4bd7-a55e-d87d25a48378"
   },
   "outputs": [],
   "source": [
    "!curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/actions.npy\n",
    "!curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/log_probs.npy\n",
    "!curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/values.npy\n",
    "!curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/observations.npy\n",
    "!curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/rewards.npy\n",
    "!curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/resets.npy\n",
    "!curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/state.npy\n",
    "!curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/advantages.npy\n",
    "!curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/value_targets.npy\n",
    "!curl -O https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/test_ppo/policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gnzN1lr248Qy",
    "outputId": "dfda0882-68c1-4081-938f-ffd815e8e3c8"
   },
   "outputs": [],
   "source": [
    "def test_gae():\n",
    "    trajectory = {}\n",
    "    for key in [\"actions\", \"log_probs\", \"values\", \"observations\", \"rewards\", \"resets\"]:\n",
    "        trajectory[key] = np.load(f\"{key}.npy\", allow_pickle=True)\n",
    "    trajectory[\"state\"] = {\"latest_observation\": np.array([np.load(\"state.npy\")])}\n",
    "\n",
    "    policy = torch.load(f\"policy\", weights_only=False)\n",
    "    policy.model = policy.model.cuda()\n",
    "    gae_to_test = GAE(policy, gamma=0.99, lambda_=0.95)\n",
    "\n",
    "    gae_to_test(trajectory)\n",
    "\n",
    "    for key in [\"advantages\", \"value_targets\"]:\n",
    "        expected = np.load(f\"{key}.npy\")\n",
    "        generated = trajectory[key]\n",
    "        assert np.allclose(expected.ravel(), generated.ravel(), atol=2e-2), key\n",
    "\n",
    "    print(\"All passed\")\n",
    "\n",
    "\n",
    "test_gae()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yce0A8x7NCV7"
   },
   "source": [
    "The main advantage of PPO over simpler policy based methods like A2C is that it is possible\n",
    "to train on the same trajectory for multiple gradient steps. The following class wraps\n",
    "an `EnvRunner`. It should call the runner to get a trajectory, then return minibatches\n",
    "from it for a number of epochs, shuffling the data before each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7tIjwbkNCV8"
   },
   "outputs": [],
   "source": [
    "def flatten_first_two_dims(arr):\n",
    "    if arr.ndim == 2:\n",
    "        return arr.reshape(-1)\n",
    "    return arr.reshape(arr.shape[0] * arr.shape[1], *arr.shape[2:])\n",
    "\n",
    "\n",
    "class TrajectorySampler:\n",
    "    \"\"\"Samples minibatches from trajectory for a number of epochs.\"\"\"\n",
    "\n",
    "    def __init__(self, runner, num_epochs, num_minibatches, transforms=None):\n",
    "        self.runner = runner\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_minibatches = num_minibatches\n",
    "        self.transforms = transforms or []\n",
    "        self.minibatch_count = 0\n",
    "        self.epoch_count = 0\n",
    "        self.trajectory = None\n",
    "\n",
    "    def shuffle_trajectory(self):\n",
    "        \"\"\"Shuffles all elements in trajectory.\n",
    "\n",
    "        Should be called at the beginning of each epoch.\n",
    "        \"\"\"\n",
    "        trajectory_len = len(self.trajectory[\"observations\"])\n",
    "\n",
    "        permutation = np.random.permutation(trajectory_len)\n",
    "        for key, value in self.trajectory.items():\n",
    "            if key != \"state\":\n",
    "                self.trajectory[key] = value[permutation]\n",
    "\n",
    "    def squeeze_trajectory(self):\n",
    "        for key, value in self.trajectory.items():\n",
    "            if key != \"state\":\n",
    "                self.trajectory[key] = flatten_first_two_dims(value)\n",
    "\n",
    "    def get_trajectory(self):\n",
    "        self.trajectory = self.runner.get_next()\n",
    "        self.squeeze_trajectory()\n",
    "\n",
    "    def get_next(self):\n",
    "        \"\"\"Returns next minibatch.\"\"\"\n",
    "        if not self.trajectory:\n",
    "            self.get_trajectory()\n",
    "\n",
    "        if self.minibatch_count == self.num_minibatches:\n",
    "            self.shuffle_trajectory()\n",
    "            self.minibatch_count = 0\n",
    "            self.epoch_count += 1\n",
    "\n",
    "        if self.epoch_count == self.num_epochs:\n",
    "            self.get_trajectory()\n",
    "            self.shuffle_trajectory()\n",
    "            self.minibatch_count = 0\n",
    "            self.epoch_count = 0\n",
    "\n",
    "        trajectory_len = self.trajectory[\"observations\"].shape[0]\n",
    "\n",
    "        batch_size = trajectory_len // self.num_minibatches\n",
    "\n",
    "        minibatch = {}\n",
    "        for key, value in self.trajectory.items():\n",
    "            if key != \"state\":\n",
    "                minibatch[key] = value[\n",
    "                    self.minibatch_count\n",
    "                    * batch_size : (self.minibatch_count + 1)\n",
    "                    * batch_size\n",
    "                ]\n",
    "\n",
    "        self.minibatch_count += 1\n",
    "\n",
    "        for transform in self.transforms:\n",
    "            transform(minibatch)\n",
    "\n",
    "        return minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1UgHPb0NCV8"
   },
   "source": [
    "A common trick to use with GAE is to normalize advantages, please implement the normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZrUlmFYNCV9"
   },
   "outputs": [],
   "source": [
    "class NormalizeAdvantages:\n",
    "    \"\"\"Normalizes advantages to have zero mean and unit std.\"\"\"\n",
    "\n",
    "    def __call__(self, trajectory):\n",
    "        #<YOUR_CODER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXnare-INCV-"
   },
   "source": [
    "Finally, we can create our PPO sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tx0Yr0GtNCV_"
   },
   "outputs": [],
   "source": [
    "def make_ppo_sampler(\n",
    "    env,\n",
    "    policy,\n",
    "    num_runner_steps=2048,\n",
    "    gamma=0.99,\n",
    "    lambda_=0.95,\n",
    "    num_epochs=10,\n",
    "    num_minibatches=32,\n",
    "):\n",
    "    \"\"\"Creates runner for PPO algorithm.\"\"\"\n",
    "    runner_transforms = [AsArray(), GAE(policy, gamma=gamma, lambda_=lambda_)]\n",
    "    runner = EnvRunner(env, policy, num_runner_steps, transforms=runner_transforms)\n",
    "\n",
    "    sampler_transforms = [NormalizeAdvantages()]\n",
    "    sampler = TrajectorySampler(\n",
    "        runner,\n",
    "        num_epochs=num_epochs,\n",
    "        num_minibatches=num_minibatches,\n",
    "        transforms=sampler_transforms,\n",
    "    )\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX1aZueWNCWA"
   },
   "source": [
    "In the next cell you will need to implement Proximal Policy Optimization algorithm itself. The algorithm\n",
    "modifies the typical policy gradient loss in the following way:\n",
    "\n",
    "$$\n",
    "J_{\\pi}(s, a) = \\frac{\\pi_\\theta(a|s)}{\\pi_\\theta^{\\text{old}}(a|s)} \\cdot A^{\\mathrm{GAE}(\\gamma,\\lambda)}(s, a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "J_{\\pi}^{\\text{clipped}}(s, a) = \\mathrm{clip}\\left(\n",
    "\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta^{\\text{old}}}(a|s)},\n",
    "1 - \\text{cliprange}, 1 + \\text{cliprange}\\right)\\cdot A^{\\mathrm{GAE(\\gamma, \\lambda)}}(s, a)\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{\\text{policy}} = -\\frac{1}{T}\\sum_{t=0}^{T-1}\\min\\left(J_\\pi(s_{t}, a_{t}), J_{\\pi}^{\\text{clipped}}(s_{t}, a_{t})\\right).\n",
    "$$\n",
    "\n",
    "The value loss is also modified:\n",
    "\n",
    "$$\n",
    "L_{V}^{\\text{clipped}} = \\frac{1}{T}\\sum_{t=0}^{T-1} \\max(l^{simple}(s_{t}), l^{clipped}(s_{t}))\n",
    "$$\n",
    ", where $l^{simple}$ is your standard critic loss\n",
    "$$\n",
    "l^{simple}(s_{t}) = [V_\\theta(s_{t}) - \\hat{V}(s_{t})]^2\n",
    "$$\n",
    "\n",
    "and $l^{clipped}$ is a clipped version that limits large changes of the value function:\n",
    "$$\n",
    "l^{clipped}(s_{t}) = [\n",
    "V_{\\theta^{\\text{old}}}(s_{t}) +\n",
    "\\text{clip}\\left(\n",
    "V_\\theta(s_{t}) - V_{\\theta^\\text{old}}(s_{t}),\n",
    "-\\text{cliprange}, \\text{cliprange}\n",
    "\\right) - \\hat{V}(s_{t})] ^ 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYGV4EmxNCWB"
   },
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(\n",
    "        self, policy, optimizer, cliprange=0.2, value_loss_coef=0.25, max_grad_norm=0.5\n",
    "    ):\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.cliprange = cliprange\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        # Note that we don't need entropy regularization for this env.\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def policy_loss(self, trajectory, act):\n",
    "        \"\"\"Computes and returns policy loss on a given trajectory.\"\"\"\n",
    "        #<YOUR_CODE>\n",
    "\n",
    "    def value_loss(self, trajectory, act):\n",
    "        \"\"\"Computes and returns value loss on a given trajectory.\"\"\"\n",
    "        #<YOUR_CODE>\n",
    "\n",
    "    def loss(self, trajectory):\n",
    "        #<YOUR_CODE>\n",
    "\n",
    "    def step(self, trajectory):\n",
    "        \"\"\"Computes the loss function and performs a single gradient step.\"\"\"\n",
    "        #<YOUR_CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVNS0IFhNCWB"
   },
   "source": [
    "Now everything is ready to do training. In one million of interactions it should be possible to\n",
    "achieve the total raw reward of about 1500. You should plot this quantity with respect to\n",
    "`runner.step_var` &mdash; the number of interactions with the environment. It is highly\n",
    "encouraged to also provide plots of the following quantities (these are useful for debugging as well):\n",
    "\n",
    "* [Coefficient of Determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) between\n",
    "value targets and value predictions\n",
    "* Entropy of the policy $\\pi$\n",
    "* Value loss\n",
    "* Policy loss\n",
    "* Value targets\n",
    "* Value predictions\n",
    "* Gradient norm\n",
    "* Advantages\n",
    "\n",
    "For optimization it is suggested to use Adam optimizer with linearly annealing learning rate\n",
    "from 3e-4 to 0 and epsilon 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmroaOHX33nZ"
   },
   "outputs": [],
   "source": [
    "state_dim = vec_env.single_observation_space.shape[0]\n",
    "action_dim = vec_env.single_action_space.shape[0]\n",
    "\n",
    "model = PolicyModel(state_dim, action_dim)\n",
    "model = model.cuda()\n",
    "\n",
    "policy = Policy(model)\n",
    "\n",
    "num_envs = 2\n",
    "vec_env = make_vec_env(num_envs=num_envs)\n",
    "sampler = make_ppo_sampler(vec_env, policy)\n",
    "\n",
    "optimizer = torch.optim.Adam(policy.model.parameters(), lr=3e-4, eps=1e-5)\n",
    "n_iter = 250000\n",
    "\n",
    "lr_mult = lambda epoch: (1 - (epoch / n_iter))\n",
    "sched = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_mult)\n",
    "\n",
    "ppo = PPO(policy, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Psvv8oru3POG"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "2W5StJsb1mz2",
    "outputId": "15b9c8ed-6704-4900-ab3b-457915874166"
   },
   "outputs": [],
   "source": [
    "steps = []\n",
    "rewards = []\n",
    "\n",
    "for i in tqdm(range(n_iter)):\n",
    "    trajectory = sampler.get_next()\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        clear_output(True)\n",
    "        rewards.append(vec_env.env.env.env.return_queue[-1])\n",
    "        steps.append(sampler.runner.step_var)\n",
    "        \n",
    "        plt.plot(steps, rewards, label=\"episode rewards\")\n",
    "        plt.title(\"Reward\")\n",
    "        plt.xlabel(\"Total steps\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    ppo.step(trajectory)\n",
    "    sched.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xQipb1w48Qz"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sixQI_jMASJR"
   },
   "outputs": [],
   "source": [
    "def evaluate(env, actor, n_games=1, t_max=1000):\n",
    "    \"\"\"\n",
    "    Plays n_games and returns rewards and rendered games\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(n_games):\n",
    "        s, _ = env.reset()\n",
    "\n",
    "        R = 0\n",
    "        for _ in range(t_max):\n",
    "            # select action for final evaluation of your policy\n",
    "            action = actor.act(np.array([s]), training=False)[\"actions\"]\n",
    "\n",
    "            s, r, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            R += r\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        rewards.append(R)\n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHPFX0eZ48Q0"
   },
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import (\n",
    "    RecordVideo,\n",
    "    NormalizeObservation as SingleEnvNormalization,\n",
    ")\n",
    "\n",
    "# let's hope this will work\n",
    "# don't forget to pray\n",
    "with gym.make(\"HalfCheetah-v5\", render_mode=\"rgb_array\") as test_env, RecordVideo(\n",
    "    env=SingleEnvNormalization(test_env), video_folder=\"./videos\"\n",
    ") as env_monitor:\n",
    "    evaluate(env_monitor, policy, n_games=5, t_max=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "gyOigVor48Q0",
    "outputId": "23b37c70-6c8b-43d8-ab45-5806fc0baf47"
   },
   "outputs": [],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "import sys\n",
    "\n",
    "video_paths = sorted([s for s in Path(\"videos\").iterdir() if s.suffix == \".mp4\"])\n",
    "video_path = video_paths[-1]  # You can also try other indices\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # https://stackoverflow.com/a/57378660/1214547\n",
    "    with video_path.open(\"rb\") as fp:\n",
    "        mp4 = fp.read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "else:\n",
    "    data_url = str(video_path)\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\n",
    "        data_url\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2HU_vKTXYmT"
   },
   "source": [
    "# Bonus area\n",
    "* Implement Recurrent PPO using LSTM and compare it with vanilla MLP PPO. (up to 3 points) See: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25nvjP_-Xbxg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
